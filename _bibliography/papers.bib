---
---

# Publications

# Preprints

@article{ohschuler24arxiv,
  author = {Oh, Byung-Doh and Schuler, William},
  title = {The impact of token granularity on the predictive power of language model surprisal},
  year = {2024},
  journal = {arXiv},
  pdf = {https://arxiv.org/abs/2412.11940},
  abbr = {arXiv},
  section = {preprints},
  bibtex_show = {true}
}

# 2025

@inproceedings{clarketal25coling,
  author = {Clark, Christian and Oh, Byung-Doh and Schuler, William},
  title = {Linear recency bias during training improves Transformers' fit to reading times},
  year = {2025},
  booktitle = {Proceedings of the 31st International Conference on Computational Linguistics},
  pages = {7735-7747},
  pdf = {https://aclanthology.org/2025.coling-main.517},
  abbr = {COLING},
  section = {papers},
  bibtex_show = {true}
}

# 2024

@inproceedings{ohschuler24emnlp,
  author = {Oh, Byung-Doh and Schuler, William},
  title = {Leading whitespaces of language models' subword vocabulary pose a confound for calculating word probabilities},
  year = {2024},
  booktitle = {Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing},
  pages = {3464--3472},
  pdf = {https://aclanthology.org/2024.emnlp-main.202},
  poster = {emnlp24_whitespace_pos.pdf},
  abbr = {EMNLP},
  section = {papers},
  selected = {true},
  bibtex_show = {true}
}

@inproceedings{ohetal24eacl,
  author = {Oh, Byung-Doh and Yue, Shisen and Schuler, William},
  title = {Frequency explains the inverse correlation of large language models' size, training data amount, and surprisal's fit to reading times},
  year = {2024},
  booktitle = {Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics},
  pages = {2644--2663},
  pdf = {https://aclanthology.org/2024.eacl-long.162},
  slides = {eacl24_frequency_slides.pdf},
  abbr = {EACL},
  section = {papers},
  selected = {true},
  bibtex_show = {true}
}

# 2023

@inproceedings{ohschuler23emnlp,
  author = {Oh, Byung-Doh and Schuler, William},
  title = {Transformer-based language model surprisal predicts human reading times best with about two billion training tokens},
  year = {2023},
  booktitle = {Findings of the Association for Computational Linguistics: EMNLP 2023},
  pages = {1915--1921},
  pdf = {https://aclanthology.org/2023.findings-emnlp.128},
  slides = {femnlp23_slmsurp_slides.pdf},
  abbr = {EMNLP Findings},
  section = {papers},
  selected = {true},
  bibtex_show = {true}
}

@inproceedings{ohschuler23acl,
  author = {Oh, Byung-Doh and Schuler, William},
  title = {Token-wise decomposition of autoregressive language model hidden states for analyzing model predictions},
  year = {2023},
  booktitle = {Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics},
  pages = {10105--10117},
  pdf = {https://aclanthology.org/2023.acl-long.562},
  poster = {acl23_decomp_pos.pdf},
  abbr = {ACL},
  section = {papers},
  selected = {true},
  bibtex_show = {true}
}

@article{ohschuler23tacl,
  author = {Oh, Byung-Doh and Schuler, William},
  title = {Why does surprisal from larger Transformer-based language models provide a poorer fit to human reading times?},
  year = {2023},
  journal = {Transactions of the Association for Computational Linguistics},
  volume = {11},
  pages = {336--350},
  pdf = {https://doi.org/10.1162/tacl_a_00548},
  slides = {tacl23_surpinv_slides.pdf},
  abbr = {TACL},
  section = {articles},
  selected = {true},
  bibtex_show = {true}
}

@inproceedings{ohschuler23hspsurp,
  author = {Oh, Byung-Doh and Schuler, William},
  title = {On the bigger-is-worse nature of pre-trained language model surprisal},
  year = {2023},
  booktitle = {36th Annual Conference on Human Sentence Processing},
  pdf = {hsp23_surpinv_abs.pdf},
  poster = {hsp23_surpinv_pos.pdf},
  abbr = {HSP},
  section = {abstracts},
  bibtex_show = {true}
}

@inproceedings{ohschuler23hspattn,
  author = {Oh, Byung-Doh and Schuler, William},
  title = {Memory-based predictors from GPT-2 attention predict reading times over surprisal},
  year = {2023},
  booktitle = {36th Annual Conference on Human Sentence Processing},
  pdf = {hsp23_attndist_abs.pdf},
  poster = {hsp23_attndist_pos.pdf},
  abbr = {HSP},
  section = {abstracts},
  bibtex_show = {true}
}

# 2022

@inproceedings{ohschuler22emnlp,
  author = {Oh, Byung-Doh and Schuler, William},
  title = {Entropy- and distance-based predictors from GPT-2 attention patterns predict reading times over and above GPT-2 surprisal},
  year = {2022},
  booktitle = {Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing},
  pages = {9324--9334},
  pdf = {https://aclanthology.org/2022.emnlp-main.632},
  poster = {emnlp22_attndist_pos.pdf},
  abbr = {EMNLP},
  section = {papers},
  selected = {true},
  bibtex_show = {true}
}

@inproceedings{oh22darpa,
  author = {Oh, Byung-Doh},
  title = {Unified unsupervised grammar induction for typologically diverse languages},
  year = {2022},
  booktitle = {DARPA Risers},
  pdf = {darpa22_induction_abs.pdf},
  poster = {darpa22_induction_pos.pdf},
  abbr = {DARPA Risers},
  section = {abstracts},
  bibtex_show = {true}
}

@article{ohetal22fai,
  author = {Oh, Byung-Doh and Clark, Christian and Schuler, William},
  title = {Comparison of structural parsers and neural language models as surprisal estimators},
  year = {2022},
  journal = {Frontiers in Artificial Intelligence},
  volume = {5},
  pages = {777963},
  pdf = {https://doi.org/10.3389/frai.2022.777963},
  abbr = {FAI},
  section = {articles},
  selected = {true},
  bibtex_show = {true}
}

# 2021

@inproceedings{jinetal21emnlp,
  author = {Jin, Lifeng and Oh, Byung-Doh and Schuler, William},
  title = {Character-based PCFG induction for modeling the syntactic acquisition of morphologically rich languages},
  year = {2021},
  booktitle = {Findings of the Association for Computational Linguistics: EMNLP 2021},
  pages = {4367--4378},
  pdf = {https://aclanthology.org/2021.findings-emnlp.371},
  abbr = {EMNLP Findings},
  section = {papers},
  bibtex_show = {true}
}

@inproceedings{jaffeetal21emnlp,
  author = {Jaffe, Evan and Oh, Byung-Doh and Schuler, William},
  title = {Coreference-aware surprisal predicts brain response},
  year = {2021},
  booktitle = {Findings of the Association for Computational Linguistics: EMNLP 2021},
  pages = {3351--3356},
  pdf = {https://aclanthology.org/2021.findings-emnlp.285},
  abbr = {EMNLP Findings},
  section = {papers},
  bibtex_show = {true}
}

@inproceedings{ohetal21acl,
  author = {Oh, Byung-Doh and Clark, Christian and Schuler, William},
  title = {Surprisal estimators for human reading times need character models},
  year = {2021},
  booktitle = {Proceedings of the Joint Conference of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing},
  pages = {3746--3757},
  pdf = {https://aclanthology.org/2021.acl-long.290},
  slides = {acl21_semproc_slides.pdf},
  abbr = {ACL},
  section = {papers},
  bibtex_show = {true}
}

@inproceedings{ohschuler21cmcl,
  author = {Oh, Byung-Doh and Schuler, William},
  title = {Contributions of propositional content and syntactic category information in sentence processing},
  year = {2021},
  booktitle = {Proceedings of the 11th Workshop on Cognitive Modeling and Computational Linguistics},
  pages = {241--250},
  pdf = {https://aclanthology.org/2021.cmcl-1.28},
  poster = {cmcl21_catcon_pos.pdf},
  abbr = {CMCL},
  section = {papers},
  bibtex_show = {true}
}

@inproceedings{oh21cmcl,
  author = {Oh, Byung-Doh},
  title = {Team Ohio State at CMCL 2021 shared task: Fine-tuned RoBERTa for eye-tracking data prediction},
  year = {2021},
  booktitle = {Proceedings of the 11th Workshop on Cognitive Modeling and Computational Linguistics},
  pages = {97-101},
  pdf = {https://aclanthology.org/2021.cmcl-1.11},
  poster = {cmcl21_st_pos.pdf},
  abbr = {CMCL},
  section = {papers},
  bibtex_show = {true}
}

@inproceedings{ohetal21hspsurp,
  author = {Oh, Byung-Doh and Clark, Christian and Schuler, William},
  title = {Comparison of structural and neural language models as surprisal estimators},
  year = {2021},
  booktitle = {34th Annual CUNY Conference on Human Sentence Processing},
  pdf = {cuny21_nlmsurp_abs.pdf},
  slides = {cuny21_nlmsurp_slides.pdf},
  abbr = {CUNY},
  section = {abstracts},
  bibtex_show = {true}
}

@inproceedings{ohetal21hspconcat,
  author = {Oh, Byung-Doh and Schuler, William},
  title = {Contributions of propositional content and syntactic categories in sentence processing},
  year = {2021},
  booktitle = {34th Annual CUNY Conference on Human Sentence Processing},
  pdf = {cuny21_catcon_abs.pdf},
  slides = {cuny21_catcon_slides.pdf},
  abbr = {CUNY},
  section = {abstracts},
  bibtex_show = {true}
}

# 2019

@inproceedings{ohetal19sigmorphon,
  author = {Oh, Byung-Doh and Maneriker, Pranav and Jiang, Nanjiang},
  title = {THOMAS: The hegemonic OSU morphological analyzer using seq2seq},
  year = {2019},
  booktitle = {Proceedings of the 16th Workshop on Computational Research in Phonetics, Phonology, and Morphology},
  pages = {80--86},
  pdf = {https://aclanthology.org/W19-4210},
  abbr = {SIGMORPHON},
  section = {abstracts},
  bibtex_show = {true}
}

@article{elsneretal19jlm,
  author = {Elsner, Micha and Sims, Andrea D. and Erdmann, Alexander and Hernandez, Antonio and Jaffe, Evan and Jin, Lifeng and Johnson, Martha Booker and Karim, Shuan and King, David L. and Lamberti Nunes, Luana and Oh, Byung-Doh and Rasmussen, Nathan and Shain, Cory and Antetomaso, Stephanie and Dickinson, Kendra V. and Diewald, Noah and McKenzie, Michelle and Stevens-Guille, Symon},
  title = {Modeling morphological learning, typology, and change: What can the neural sequence-to-sequence framework contribute?},
  year = {2019},
  journal = {Journal of Language Modelling},
  volume = {7},
  number = {1},
  pages = {53--98},
  pdf = {https://doi.org/10.15398/jlm.v7i1.244},
  abbr = {JLM},
  section = {articles},
  bibtex_show = {true}
}

@inproceedings{jaffeoh19aimm,
  author = {Jaffe, Evan and Oh, Byung-Doh},
  title = {The role of learnability in morphological change: A computational approach},
  year = {2019},
  booktitle = {Fourth American International Morphology Meeting},
  pdf = {aimm4_chakavian_abs.pdf},
  abbr = {AIMM},
  section = {abstracts},
  bibtex_show = {true}
}

# 2018

@article{ohso18et,
  author = {Oh, Byung-Doh and So, Youngsoon},
  title = {Exploring English online research and comprehension strategies of Korean college students},
  year = {2018},
  journal = {English Teaching},
  volume = {73},
  number = {3},
  pages = {53--76},
  pdf = {https://doi.org/10.15858/engtea.73.3.201809.53},
  abbr = {Eng Tea},
  section = {articles},
  bibtex_show = {true}
}

# 2017

@article{oh17fler,
  author = {Oh, Byung-Doh},
  title = {Predicting L2 writing proficiency with computational indices based on n-grams},
  year = {2017},
  journal = {Foreign Language Education Research},
  volume = {21},
  pages = {1-20},
  pdf = {https://hdl.handle.net/10371/139753},
  abbr = {FLER},
  section = {articles},
  bibtex_show = {true}
}

# Talks

@article{ohtalkpostech24,
    title = {What can linguistic data tell us about the predictions of large language models?},
    journal = {Department of Computer Science/Graduate School of AI, POSTECH},
    year = {2024},
    slides = {postech_241007.pdf},
    abbr = {Talk},
    section = {talks}
}

@article{ohtalkuds24,
    title = {The bigger-is-worse effects of model size and training data of large language model surprisal on human reading times},
    journal = {Distinguished Speakers in Language Science Colloquium Series, Saarland University},
    year = {2024},
    slides = {uds_240425.pdf},
    abbr = {Talk},
    section = {talks}
}

@article{ohtalknyu23,
    title = {The bigger-is-worse effects of model size and training data of large language model surprisal on human reading times},
    journal = {Center for Data Science, New York University},
    year = {2023},
    slides = {nyu_231109.pdf},
    abbr = {Talk},
    section = {talks}
}

@article{ohtalkdu22,
    title = {Computational models of sentence processing and syntactic acquisition},
    journal = {Department of English, Dongguk University (Online)},
    year = {2022},
    slides = {du_220214.pdf},
    abbr = {Talk},
    section = {talks}
}

# Teaching

@article{ohteachingtextasdata25,
    title = {DS-GA 1015: Text as Data},
    journal = {Instructor of Record, New York University, Spring},
    year = {2025},
    syllabus = {sp25_textasdata.pdf},
    abbr = {Teaching},
    section = {teaching}
}

@article{ohteachingclcs24,
    title = {Transformer-based language model surprisal predicts human reading times best with about two billion training tokens},
    journal = {Invited Lecture, Computational Linguistics and Cognitive Science, New York University (Instructor: Tal Linzen)},
    year = {2024},
    abbr = {Teaching},
    section = {teaching}
}

@article{ohteachingmetro24,
    title = {Guessing meaning and breaking it down (with NACLO problems)},
    journal = {Invited Lecture, Metro Early College High School},
    year = {2024},
    slides = {metro_240124.pdf},
    abbr = {Teaching},
    section = {teaching}
}

@article{ohteachingcodebreaking21,
    title = {LING 3801: Codes and Code Breaking},
    journal = {Instructor of Record, The Ohio State University, Spring},
    year = {2021},
    syllabus = {sp21_codebreaking.pdf},
    abbr = {Teaching},
    section = {teaching}
}

@article{ohteachingcodebreaking20,
    title = {LING 3801: Codes and Code Breaking},
    journal = {Instructor of Record, The Ohio State University, Autumn},
    year = {2020},
    syllabus = {au20_codebreaking.pdf},
    abbr = {Teaching},
    section = {teaching}
}

@article{ohteachingcolenglish17,
    title = {College English 1},
    journal = {Teaching Assistant, Seoul National University, Summer},
    year = {2017},
    abbr = {Teaching},
    section = {teaching}
}
